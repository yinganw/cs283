{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk4gccjTw4Mx"
      },
      "source": [
        "# HW4 (c): Text-to-Speech with Language Models (40 points)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this assignment, you will implement and train a Language Model (LM) for Text-to-Speech (TTS) synthesis, following the CosyVoice2 architecture. The assignment focuses on the core component of modern TTS systems: autoregressive language modeling for speech token generation.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "* Understand modern TTS architecture and how LLM-based TTS systems work\n",
        "* Implement autoregressive generation with a transformer that generates speech tokens from text\n",
        "* Handle multi-modal sequences by working with text and speech tokens in a unified framework\n",
        "* Train a language model from scratch on real speech data\n",
        "* Implement zero-shot voice cloning with in-context learning\n",
        "* Evaluate TTS quality using Word Error Rate (WER) with automatic speech recognition\n",
        "\n",
        "### System Architecture\n",
        "\n",
        "```\n",
        "Text → [Text Tokenizer] → Text Tokens → [Your LM] → Speech Tokens → [Flow Model] → Mel → [Vocoder] → Audio\n",
        "```\n",
        "\n",
        "You will implement and train the Language Model component that converts text tokens to speech tokens. All other components (tokenizers, flow model, vocoder) are provided as pre-trained models.\n",
        "\n",
        "### Assignment Package\n",
        "\n",
        "Download [**hw4_util.zip**](https://drive.google.com/file/d/1q5OdFHgXBtdK5MHJCWmWeyBHmz4Y-QVy/view?usp=sharing) and upload it to your Google Drive.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "The assignment uses the LibriTTS dataset (pre-tokenized for efficiency):\n",
        "* 354,780 training samples\n",
        "* 9,957 test samples\n",
        "* Multi-speaker data for diverse voice generation\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [CosyVoice2 Paper](https://arxiv.org/abs/2412.10117)\n",
        "- [LibriTTS Dataset](https://www.openslr.org/60/)\n",
        "- [Transformer Architecture](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "### Submission Requirements\n",
        "\n",
        "You will submit **only 2 files** to Gradescope:\n",
        "\n",
        "1. **`submission_[YOUR_ID].txt`** - Auto-generated WER evaluation results from Part 8\n",
        "2. **`hw4-c.pdf`** - PDF export of this notebook showing all your code and outputs\n",
        "\n",
        "**Important Notes:**\n",
        "* The submission file is automatically generated when you run Part 8's evaluation\n",
        "* Do NOT modify the submission file format - it must match the exact format for autograding\n",
        "* Your WER score will be calculated on Gradescope using your transcriptions\n",
        "* Ensure all code cells have been executed and outputs are visible in the PDF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SE-e66zw4My"
      },
      "source": [
        "## Part 0: Environment Setup\n",
        "\n",
        "The cell below will automatically detect your environment (Google Colab or Local Machine) and set it up accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r hw4_util/requirements.txt"
      ],
      "metadata": {
        "id": "PRrD_dbVJCqm",
        "outputId": "eb5273e9-389d-47bc-e458-c573cd213579",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 5)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 6)) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 9)) (4.57.1)\n",
            "Requirement already satisfied: diffusers==0.29.0 in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 10)) (0.29.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 11)) (1.11.0)\n",
            "Requirement already satisfied: onnxruntime-gpu>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 14)) (1.23.2)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 17)) (0.13.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 18)) (0.11.0)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 19)) (20250625)\n",
            "Requirement already satisfied: pyworld in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 20)) (0.3.5)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 23)) (4.0.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 24)) (0.4.6)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 27)) (5.49.1)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 30)) (2.3.0)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 31)) (1.3.2)\n",
            "Requirement already satisfied: conformer==0.3.2 in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 32)) (0.3.2)\n",
            "Requirement already satisfied: lightning in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 33)) (2.5.6)\n",
            "Requirement already satisfied: modelscope in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 34)) (1.31.0)\n",
            "Requirement already satisfied: wetext==0.0.4 in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 35)) (0.0.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 38)) (13.9.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 39)) (5.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 40)) (4.67.1)\n",
            "Requirement already satisfied: wget==3.2 in /usr/local/lib/python3.12/dist-packages (from -r hw4_util/requirements.txt (line 41)) (3.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from diffusers==0.29.0->-r hw4_util/requirements.txt (line 10)) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers==0.29.0->-r hw4_util/requirements.txt (line 10)) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.29.0->-r hw4_util/requirements.txt (line 10)) (0.36.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers==0.29.0->-r hw4_util/requirements.txt (line 10)) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.29.0->-r hw4_util/requirements.txt (line 10)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers==0.29.0->-r hw4_util/requirements.txt (line 10)) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.29.0->-r hw4_util/requirements.txt (line 10)) (0.6.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers==0.29.0->-r hw4_util/requirements.txt (line 10)) (11.3.0)\n",
            "Requirement already satisfied: einops>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from conformer==0.3.2->-r hw4_util/requirements.txt (line 32)) (0.8.1)\n",
            "Requirement already satisfied: kaldifst in /usr/local/lib/python3.12/dist-packages (from wetext==0.0.4->-r hw4_util/requirements.txt (line 35)) (1.7.17)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->-r hw4_util/requirements.txt (line 9)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->-r hw4_util/requirements.txt (line 9)) (6.0.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.40.0->-r hw4_util/requirements.txt (line 9)) (0.22.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r hw4_util/requirements.txt (line 11)) (5.9.5)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu>=1.18.0->-r hw4_util/requirements.txt (line 14)) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu>=1.18.0->-r hw4_util/requirements.txt (line 14)) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu>=1.18.0->-r hw4_util/requirements.txt (line 14)) (5.29.5)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile->-r hw4_util/requirements.txt (line 17)) (2.0.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->-r hw4_util/requirements.txt (line 18)) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r hw4_util/requirements.txt (line 18)) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r hw4_util/requirements.txt (line 18)) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r hw4_util/requirements.txt (line 18)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r hw4_util/requirements.txt (line 18)) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r hw4_util/requirements.txt (line 18)) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r hw4_util/requirements.txt (line 18)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->-r hw4_util/requirements.txt (line 18)) (1.0.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->-r hw4_util/requirements.txt (line 18)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->-r hw4_util/requirements.txt (line 18)) (1.1.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r hw4_util/requirements.txt (line 19)) (10.8.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper->-r hw4_util/requirements.txt (line 19)) (0.12.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer->-r hw4_util/requirements.txt (line 23)) (8.3.0)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.12/dist-packages (from jiwer->-r hw4_util/requirements.txt (line 23)) (3.14.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate->-r hw4_util/requirements.txt (line 24)) (4.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate->-r hw4_util/requirements.txt (line 24)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate->-r hw4_util/requirements.txt (line 24)) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate->-r hw4_util/requirements.txt (line 24)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate->-r hw4_util/requirements.txt (line 24)) (0.70.16)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (0.121.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (3.11.4)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (0.14.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio->-r hw4_util/requirements.txt (line 27)) (0.38.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio->-r hw4_util/requirements.txt (line 27)) (15.0.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->-r hw4_util/requirements.txt (line 30)) (4.9.3)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from lightning->-r hw4_util/requirements.txt (line 33)) (0.15.2)\n",
            "Requirement already satisfied: torchmetrics<3.0,>0.7.0 in /usr/local/lib/python3.12/dist-packages (from lightning->-r hw4_util/requirements.txt (line 33)) (1.8.2)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.12/dist-packages (from lightning->-r hw4_util/requirements.txt (line 33)) (2.5.6)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.12/dist-packages (from modelscope->-r hw4_util/requirements.txt (line 34)) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->-r hw4_util/requirements.txt (line 38)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->-r hw4_util/requirements.txt (line 38)) (2.19.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown->-r hw4_util/requirements.txt (line 39)) (4.13.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio->-r hw4_util/requirements.txt (line 27)) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio->-r hw4_util/requirements.txt (line 27)) (1.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile->-r hw4_util/requirements.txt (line 17)) (2.23)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate->-r hw4_util/requirements.txt (line 24)) (18.1.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio->-r hw4_util/requirements.txt (line 27)) (0.0.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate->-r hw4_util/requirements.txt (line 24)) (3.13.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r hw4_util/requirements.txt (line 27)) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio->-r hw4_util/requirements.txt (line 27)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio->-r hw4_util/requirements.txt (line 27)) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.23.2->diffusers==0.29.0->-r hw4_util/requirements.txt (line 10)) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->-r hw4_util/requirements.txt (line 38)) (0.1.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa->-r hw4_util/requirements.txt (line 18)) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate->-r hw4_util/requirements.txt (line 24)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate->-r hw4_util/requirements.txt (line 24)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate->-r hw4_util/requirements.txt (line 24)) (2025.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->-r hw4_util/requirements.txt (line 18)) (4.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio->-r hw4_util/requirements.txt (line 27)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio->-r hw4_util/requirements.txt (line 27)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio->-r hw4_util/requirements.txt (line 27)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.29.0->-r hw4_util/requirements.txt (line 10)) (3.4.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa->-r hw4_util/requirements.txt (line 18)) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.3.0->-r hw4_util/requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio->-r hw4_util/requirements.txt (line 27)) (1.5.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown->-r hw4_util/requirements.txt (line 39)) (2.8)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime-gpu>=1.18.0->-r hw4_util/requirements.txt (line 14)) (10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->diffusers==0.29.0->-r hw4_util/requirements.txt (line 10)) (3.23.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r hw4_util/requirements.txt (line 39)) (1.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate->-r hw4_util/requirements.txt (line 24)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate->-r hw4_util/requirements.txt (line 24)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate->-r hw4_util/requirements.txt (line 24)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate->-r hw4_util/requirements.txt (line 24)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate->-r hw4_util/requirements.txt (line 24)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate->-r hw4_util/requirements.txt (line 24)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate->-r hw4_util/requirements.txt (line 24)) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate->-r hw4_util/requirements.txt (line 24)) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/hw4_util/* /content/"
      ],
      "metadata": {
        "id": "qZkjYWl1JQyu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyJFWp7Yw-CX",
        "outputId": "6c67d6fb-c018-454f-81d9-4e6f528335e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Detected: Google Colab\n",
            "\n",
            "Checking core dependencies...\n",
            "  [OK] PyTorch 2.8.0+cu126\n",
            "  [OK] GPU: Tesla T4\n",
            "  [OK] Transformers 4.57.1\n",
            "  [OK] Gradio\n",
            "  [OK] CosyVoice/ found\n",
            "\n",
            "======================================================================\n",
            "Environment Check: READY\n",
            "======================================================================\n",
            "\n",
            "You can proceed with the assignment!\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# !unzip /content/drive/MyDrive/hw4_util.zip -d /content\n",
        "# !mv /content/hw4_submission/* /content/\n",
        "# !pip install -r requirements.txt\n",
        "\n",
        "# import importlib\n",
        "# hw4_util = importlib.machinery.SourceFileLoader(\"hw4_util\", \"./hw4_util.py\").load_module()\n",
        "\n",
        "# Environment Verification\n",
        "from hw4_util import check_environment\n",
        "\n",
        "check_environment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZtKVQOfw4Mz"
      },
      "source": [
        "## Part 1: Setup and Configuration\n",
        "\n",
        "In this part, you will set up the environment and define the configuration for the assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPHbRem5w4Mz"
      },
      "outputs": [],
      "source": [
        "# Essential imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import unpad_sequence, pad_sequence\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Tuple\n",
        "from IPython.display import Audio, display\n",
        "import torchaudio\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "# Check GPU availability\n",
        "assert torch.cuda.is_available(), \"GPU is required for this assignment!\"\n",
        "device = torch.device('cuda')\n",
        "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLQ0pmj4w4Mz"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "class Config:\n",
        "    \"\"\"Essential configuration for HW4 - Paths and Fixed Tokens Only\"\"\"\n",
        "\n",
        "    # Paths (Fixed - Do Not Change)\n",
        "    DATA_CACHE_DIR = './libritts_token_cache'\n",
        "    TRAIN_CACHE = 'libritts_train.pt'\n",
        "    TEST_CACHE = 'libritts_test.pt'\n",
        "    PRETRAINED_DIR = './pretrained_models'\n",
        "    RESULTS_DIR = './results'\n",
        "\n",
        "    # Special tokens (Fixed - Matching CosyVoice2)\n",
        "    IGNORE_ID = -1        # Padding/ignore token for loss\n",
        "    SOS_EOS_ID = 0        # Start/End of sequence token\n",
        "    TASK_ID = 1           # Task separator token\n",
        "\n",
        "    # Device configuration\n",
        "    DEVICE = 'cuda'\n",
        "\n",
        "# Create necessary directories\n",
        "import os\n",
        "os.makedirs(Config.RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(Config.PRETRAINED_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Configuration loaded\")\n",
        "print(f\"Data directory: {Config.DATA_CACHE_DIR}\")\n",
        "print(f\"Results directory: {Config.RESULTS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp7YgU5Qw4Mz"
      },
      "source": [
        "## Part 2: Load Pretrained Components\n",
        "\n",
        "In this section, you will load the pre-trained [CosyVoice2](https://arxiv.org/abs/2412.10117) components that remain frozen during training:\n",
        "\n",
        "* **Text Tokenizer**: Qwen2 BPE tokenizer for converting text to tokens\n",
        "* **Speech Tokenizer**: VQ-VAE for converting audio to discrete speech tokens  \n",
        "* **Flow Matching Model**: For converting speech tokens to mel-spectrograms (inference only)\n",
        "* **Vocoder**: HiFi-GAN for converting mel-spectrograms to audio waveforms (inference only)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a3dpa2Pw4Mz"
      },
      "outputs": [],
      "source": [
        "# Import the pretrained model utilities\n",
        "# Note: This module is provided and should NOT be modified\n",
        "from hw4_util import (\n",
        "    download_pretrained_models,\n",
        "    load_text_tokenizer,\n",
        "    load_speech_tokenizer,\n",
        "    load_flow_model,\n",
        "    load_vocoder\n",
        ")\n",
        "\n",
        "# Download pretrained models if needed\n",
        "print(\"Downloading pretrained CosyVoice2 models...\")\n",
        "model_dir = download_pretrained_models(Config.PRETRAINED_DIR)\n",
        "print(f\"Models ready at: {model_dir}\")\n",
        "\n",
        "# Load tokenizers (needed for training)\n",
        "print(\"\\nLoading tokenizers for training...\")\n",
        "text_tokenizer = load_text_tokenizer(model_dir)\n",
        "speech_tokenizer = load_speech_tokenizer(model_dir, device)\n",
        "\n",
        "print(f\"Text tokenizer loaded\")\n",
        "print(f\"Vocab size: {text_tokenizer.vocab_size}\")\n",
        "print(f\"Speech tokenizer loaded\")\n",
        "print(f\"Vocab size: {speech_tokenizer.vocab_size}\")\n",
        "\n",
        "# Note: Flow model and vocoder will be loaded later for inference only\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJO-OBQpw4Mz"
      },
      "source": [
        "## Part 3: Dataset and Data Loading\n",
        "\n",
        "In this part, you will implement a custom dataset class for the LibriTTS data. The dataset uses pre-tokenized speech data for efficiency.\n",
        "\n",
        "### Requirements:\n",
        "* Implement text tokenization (on-the-fly)\n",
        "* Load pre-computed speech tokens from cache\n",
        "* Handle multi-speaker information\n",
        "* Implement proper sequence padding and batching\n",
        "\n",
        "**TODO:** Complete the `__getitem__` method in the CosyVoiceDataset class below.\n",
        "\n",
        "**TODO:** Implement the `cosyvoice_collate_fn` function for batching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-NF8y1Uw4Mz"
      },
      "outputs": [],
      "source": [
        "class CosyVoiceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Multi-speaker TTS Dataset for CosyVoice2 using pre-tokenized LibriTTS data\n",
        "\n",
        "    This dataset uses pre-computed speech tokens from cache, avoiding\n",
        "    on-the-fly audio processing for faster training.\n",
        "    \"\"\"\n",
        "    def __init__(self, samples_list, text_tokenizer, speaker_to_idx_dict,\n",
        "                 max_text_len=200, max_speech_len=500):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            samples_list: List of pre-processed samples from .pt file\n",
        "            text_tokenizer: Pretrained text tokenizer\n",
        "            speaker_to_idx_dict: Speaker ID to index mapping from metadata\n",
        "            max_text_len: Maximum text token length\n",
        "            max_speech_len: Maximum speech token length\n",
        "        \"\"\"\n",
        "        self.samples = samples_list\n",
        "        self.text_tokenizer = text_tokenizer\n",
        "        self.speaker_to_idx = speaker_to_idx_dict\n",
        "        self.max_text_len = max_text_len\n",
        "        self.max_speech_len = max_speech_len\n",
        "        self.num_speakers = len(speaker_to_idx_dict)\n",
        "\n",
        "        print(f\"  Dataset initialized with {len(self.samples)} samples\")\n",
        "        print(f\"  Number of speakers: {self.num_speakers}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single sample\n",
        "\n",
        "        Returns:\n",
        "            dict with keys: 'utt', 'text', 'text_token', 'speech_token',\n",
        "                           'speaker_idx', 'speaker_id'\n",
        "        \"\"\"\n",
        "        # TODO: Implement the __getitem__ method\n",
        "        #\n",
        "        # Tasks:\n",
        "        # 1. Get sample from self.samples[idx]\n",
        "        # 2. Extract: text, speaker_id, speech_tokens (pre-computed), utt_id\n",
        "        # 3. Return None if text or speech_tokens are invalid/empty\n",
        "        # 4. Get speaker index from self.speaker_to_idx dictionary\n",
        "        # 5. Tokenize text and truncate to max_text_len\n",
        "        # 6. Convert speech_tokens to tensor and truncate to max_speech_len\n",
        "        # 7. Return dict with required keys\n",
        "\n",
        "        pass  # TODO: Replace with your implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfPgLcQVw4M0"
      },
      "outputs": [],
      "source": [
        "def cosyvoice_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function for batching variable-length sequences.\n",
        "    Adapted from CosyVoice's collate strategy.\n",
        "\n",
        "    Key features:\n",
        "    - Filters invalid samples (None)\n",
        "    - Sorts by speech length (descending) to minimize padding\n",
        "    - Pads sequences efficiently\n",
        "    \"\"\"\n",
        "    # TODO: Implement the collate function\n",
        "    #\n",
        "    # Tasks:\n",
        "    # 1. Filter out None samples\n",
        "    # 2. Sort samples by speech_token length (descending)\n",
        "    # 3. Extract and reorder: utts, text, speaker_indices\n",
        "    # 4. Pad text_tokens and speech_tokens to same length within batch\n",
        "    # 5. Create length tensors for actual sequence lengths\n",
        "    #\n",
        "    # Use padding_value=0 for token sequences\n",
        "    # Return dict with keys: utts, text, text_tokens, text_lengths,\n",
        "    #                        speech_tokens, speech_lengths, speaker_indices\n",
        "\n",
        "    pass  # TODO: Replace with your implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yejIqpdWw4M0"
      },
      "outputs": [],
      "source": [
        "# Load pre-tokenized LibriTTS data\n",
        "print(\"Loading LibriTTS dataset from cache...\")\n",
        "train_cache = torch.load(f'{Config.DATA_CACHE_DIR}/{Config.TRAIN_CACHE}')\n",
        "test_cache = torch.load(f'{Config.DATA_CACHE_DIR}/{Config.TEST_CACHE}')\n",
        "\n",
        "train_data = train_cache['samples']\n",
        "val_data = test_cache['samples']\n",
        "metadata = train_cache['metadata']\n",
        "speaker_to_idx = metadata['speaker_to_idx']\n",
        "speech_vocab_size = metadata['speech_vocab_size']\n",
        "\n",
        "print(f\"Dataset loaded:\")\n",
        "print(f\"Train samples: {len(train_data)}\")\n",
        "print(f\"Test samples: {len(val_data)}\")\n",
        "print(f\"Speech vocab size: {speech_vocab_size}\")\n",
        "print(f\"Number of speakers: {metadata['num_speakers']}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CosyVoiceDataset(train_data, text_tokenizer, speaker_to_idx)\n",
        "val_dataset = CosyVoiceDataset(val_data, text_tokenizer, speaker_to_idx)\n",
        "\n",
        "# Create dataloaders\n",
        "# You may change the batch size, num_workers, etc. for faster training\n",
        "# But it depends on your GPU memory\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=cosyvoice_collate_fn,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=2,\n",
        "    persistent_workers=False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=cosyvoice_collate_fn,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=2,\n",
        "    persistent_workers=False\n",
        ")\n",
        "\n",
        "print(f\"DataLoaders created:\")\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7PiLw3aw4M0"
      },
      "source": [
        "## Part 4: Model Architecture\n",
        "\n",
        "In this part, you will implement the **TextToSpeechLM** model - the core component of the TTS system.\n",
        "\n",
        "### Model Requirements:\n",
        "1. Takes text tokens as input\n",
        "2. Uses transformer layers to process the sequence\n",
        "3. Generates speech tokens autoregressively\n",
        "4. Follows the CosyVoice2 sequence format: `[SOS, text_tokens, TASK_ID, speech_tokens]`\n",
        "\n",
        "**TODO:** Complete the model architecture with proper embeddings and transformer layers.\n",
        "\n",
        "**TODO:** Implement the forward pass with attention masking and loss computation.\n",
        "\n",
        "**TODO:** Implement the generate method for autoregressive inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lefGLYmpw4M0"
      },
      "outputs": [],
      "source": [
        "class TextToSpeechLM(nn.Module):\n",
        "    \"\"\"Student implementation of text to speech token generation model\n",
        "\n",
        "    Architecture:\n",
        "    - Text tokens → Text embeddings → Transformer → Speech tokens\n",
        "    - Special tokens: SOS_EOS (id=0), TASK_ID (id=1)\n",
        "    - Sequence format: [SOS, text_tokens, TASK_ID, speech_tokens]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 text_vocab_size: int,\n",
        "                 speech_vocab_size: int,\n",
        "                 d_model: int = 768,\n",
        "                 n_heads: int = 12,\n",
        "                 n_layers: int = 12,\n",
        "                 max_seq_len: int = 2048):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.speech_vocab_size = speech_vocab_size\n",
        "\n",
        "        # Special token IDs\n",
        "        self.sos_eos_id = Config.SOS_EOS_ID\n",
        "        self.task_id = Config.TASK_ID\n",
        "\n",
        "        # TODO: Define the model architecture\n",
        "        #\n",
        "        # Components needed:\n",
        "        # 1. Text embedding layer (vocab_size → d_model)\n",
        "        # 2. Speech embedding layer (vocab_size+1 → d_model, +1 for EOS)\n",
        "        # 3. Special token embeddings (2 tokens)\n",
        "        # 4. Positional encoding (learnable parameters)\n",
        "        # 5. Transformer encoder stack\n",
        "        # 6. Output projection (d_model → speech_vocab_size+1)\n",
        "\n",
        "        pass  # TODO: Replace with your implementation\n",
        "\n",
        "        # Loss function (provided)\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=Config.IGNORE_ID)\n",
        "\n",
        "    def prepare_sequence(self, text_tokens, text_lengths, speech_tokens=None, speech_lengths=None):\n",
        "        \"\"\"Prepare input sequence in CosyVoice2 format\n",
        "\n",
        "        Args:\n",
        "            text_tokens: Text token ids [B, T_text]\n",
        "            text_lengths: Actual lengths of text [B]\n",
        "            speech_tokens: Speech token ids [B, T_speech] (training only)\n",
        "            speech_lengths: Actual lengths of speech [B] (training only)\n",
        "\n",
        "        Returns:\n",
        "            lm_input: Model input embeddings\n",
        "            lm_target: Target token ids for loss computation\n",
        "            padding_mask: Boolean mask for padded positions\n",
        "        \"\"\"\n",
        "        # TODO: Implement sequence preparation\n",
        "        #\n",
        "        # Tasks:\n",
        "        # 1. Get embeddings for text, speech (if training), and special tokens\n",
        "        # 2. Build input sequence: [SOS, text_emb, TASK, speech_emb]\n",
        "        # 3. Build target sequence for teacher forcing\n",
        "        # 4. Handle variable lengths using unpad/pad operations\n",
        "        # 5. Create padding mask (True where padded)\n",
        "        #\n",
        "        # Note: Target should be shifted for next-token prediction\n",
        "        # Note: Use Config.IGNORE_ID for positions to ignore in loss\n",
        "\n",
        "        pass  # TODO: Replace with your implementation\n",
        "\n",
        "    def forward(self, text_tokens, text_lengths, speech_tokens, speech_lengths):\n",
        "        \"\"\"Forward pass for training\n",
        "\n",
        "        Args:\n",
        "            text_tokens: [B, T_text] padded text tokens\n",
        "            text_lengths: [B] actual lengths\n",
        "            speech_tokens: [B, T_speech] padded speech tokens\n",
        "            speech_lengths: [B] actual lengths\n",
        "        \"\"\"\n",
        "        # TODO: Implement forward pass\n",
        "        #\n",
        "        # Steps:\n",
        "        # 1. Prepare sequences using prepare_sequence\n",
        "        # 2. Add positional encoding to embeddings\n",
        "        # 3. Create causal mask for autoregressive modeling\n",
        "        # 4. Pass through transformer with both masks\n",
        "        # 5. Project to output vocabulary\n",
        "        # 6. Compute loss using targets from prepare_sequence\n",
        "        # 7. Compute accuracy: (correct predictions) / (non-ignored positions)\n",
        "        #\n",
        "        # Return: loss (scalar), accuracy (scalar)\n",
        "\n",
        "        pass  # TODO: Replace with your implementation\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, text_tokens, max_length=500, temperature=1.0, top_k=50,\n",
        "                 prompt_speech_token=None, prompt_text_tokens=None, min_length=None):\n",
        "        \"\"\"Generate speech tokens autoregressively\n",
        "\n",
        "        Args:\n",
        "            text_tokens: Input text [1, T]\n",
        "            max_length: Maximum generation length\n",
        "            temperature: Sampling temperature\n",
        "            top_k: Top-k sampling\n",
        "            prompt_speech_token: Optional voice prompt\n",
        "            prompt_text_tokens: Optional text for voice prompt\n",
        "        \"\"\"\n",
        "        # TODO: Implement autoregressive generation\n",
        "        #\n",
        "        # Steps:\n",
        "        # 1. Build initial sequence with special tokens\n",
        "        # 2. Add voice prompt if provided (for voice cloning)\n",
        "        # 3. Generation loop:\n",
        "        #    - Add positional encoding\n",
        "        #    - Create causal mask\n",
        "        #    - Forward through transformer\n",
        "        #    - Get logits for last position\n",
        "        #    - Apply temperature and top-k sampling\n",
        "        #    - Stop at EOS token (speech_vocab_size)\n",
        "        # 4. Return generated token ids\n",
        "        #\n",
        "        # Note: Prevent EOS before min_length tokens\n",
        "\n",
        "        pass  # TODO: Replace with your implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFvrsUDCw4M0"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "print(\"Initializing TextToSpeechLM...\")\n",
        "model = TextToSpeechLM(\n",
        "    text_vocab_size=text_tokenizer.vocab_size,\n",
        "    speech_vocab_size=speech_vocab_size\n",
        ").to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {num_params:,} ({num_params/1e6:.1f}M)\")\n",
        "print(f\"Trainable: {trainable:,} ({trainable/1e6:.1f}M)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XnQZh1Xw4M1"
      },
      "source": [
        "## Part 5: Sanity Check - Verify Data Pipeline\n",
        "\n",
        "Before training, verify that your data processing pipeline works correctly by generating audio from ground-truth speech tokens. This section helps you debug any issues before starting the training process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbXz71qow4M1"
      },
      "outputs": [],
      "source": [
        "# Audio generation helper (imported from hw4_util)\n",
        "from hw4_util import generate_audio_from_tokens\n",
        "\n",
        "# Run sanity check - generate audio from ground-truth tokens\n",
        "print(\"Running sanity check...\")\n",
        "\n",
        "# Load flow model and vocoder for sanity check\n",
        "print(\"Loading flow model and vocoder for sanity check...\")\n",
        "flow_model = load_flow_model(model_dir, device)\n",
        "vocoder = load_vocoder(model_dir, device)\n",
        "\n",
        "# Get a sample from the dataset\n",
        "sample_idx = 0\n",
        "sample = val_dataset[sample_idx]\n",
        "\n",
        "if sample is not None:\n",
        "    print(f\"\\n Sample text: '{sample['text']}'\")\n",
        "    print(f\"  Text tokens: {sample['text_token'].shape}\")\n",
        "    print(f\"  Speech tokens: {sample['speech_token'].shape}\")\n",
        "\n",
        "    # Generate audio from ground-truth speech tokens\n",
        "    audio, mel = generate_audio_from_tokens(\n",
        "        sample['speech_token'],\n",
        "        flow_model,\n",
        "        vocoder,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    sample_rate = 24000\n",
        "    duration = audio.shape[0] / sample_rate\n",
        "\n",
        "    print(f\"\\n Generated audio from ground-truth tokens:\")\n",
        "    print(f\"  Mel shape: {mel.shape}\")\n",
        "    print(f\"  Audio shape: {audio.shape}\")\n",
        "    print(f\"  Duration: {duration:.2f}s @ {sample_rate}Hz\")\n",
        "\n",
        "    # Save for listening\n",
        "    output_path = f'{Config.RESULTS_DIR}/sanity_check.wav'\n",
        "    torchaudio.save(output_path, audio.cpu().unsqueeze(0), sample_rate)\n",
        "    print(f\"  Saved to: {output_path}\")\n",
        "\n",
        "    # Display audio in notebook\n",
        "    display(Audio(audio.cpu().numpy(), rate=sample_rate))\n",
        "\n",
        "    print(\"\\n Sanity check passed! Data pipeline is working correctly.\")\n",
        "else:\n",
        "    print(\"Sample is None - check dataset configuration\")\n",
        "\n",
        "# Clean up to save memory\n",
        "del flow_model\n",
        "del vocoder\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY0iAK48w4M1"
      },
      "source": [
        "## Part 6: Training Loop\n",
        "\n",
        "In this section, you will train the TextToSpeechLM model.\n",
        "\n",
        "### Compute Requirements and Training Time\n",
        "\n",
        "**Hardware Requirements:**\n",
        "- **GPU Required**: This assignment requires a GPU with at least 16GB VRAM (e.g., NVIDIA T4, V100, A100, or similar)\n",
        "- Training will not work on CPU-only machines due to memory and speed constraints\n",
        "\n",
        "**Expected Training Time:**\n",
        "- With default batch size (4), training typically takes **8-12 hours** on a T4 GPU for a full training run (3 epochs)\n",
        "- Training time depends on:\n",
        "  - GPU model\n",
        "  - Batch size (larger batches = faster training but more memory)\n",
        "  - Number of epochs\n",
        "  - Data loading efficiency (num_workers)\n",
        "- **Note**: If you have better GPUs like V100 or A100 with more VRAM, feel free to increase the batch size for faster training\n",
        "\n",
        "**Checkpointing:**\n",
        "- **It is strongly recommended to implement checkpointing** to save your model periodically during training\n",
        "- This allows you to:\n",
        "  - Resume training if interrupted (e.g., Colab disconnects, GPU timeout)\n",
        "  - Save the best model based on validation loss\n",
        "  - Avoid losing progress if training crashes\n",
        "- Use the `save_checkpoint` helper function from `hw4_util.py` to save checkpoints after each epoch (or every N epochs)\n",
        "- The helper function saves model state, optimizer state, scheduler state, epoch number, and losses for full resumability\n",
        "- Consider saving checkpoints to Google Drive if using Colab to persist across sessions\n",
        "\n",
        "### Training Components:\n",
        "1. Setting up the optimizer and learning rate scheduler\n",
        "2. Training for multiple epochs\n",
        "3. Validating after each epoch\n",
        "4. Saving the best model\n",
        "\n",
        "**TODO:** Complete the training loop with proper loss computation and backpropagation.\n",
        "\n",
        "**TODO:** Implement validation loop with metric tracking.\n",
        "\n",
        "**TODO:** Add checkpointing to save the best model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxLzgbujw4M1"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    # TODO: Implement training loop\n",
        "    #\n",
        "    # For each batch:\n",
        "    # 1. Move data to device (GPU)\n",
        "    # 2. Forward pass to get loss and accuracy\n",
        "    # 3. Backward pass (zero_grad → backward → clip_grad → step)\n",
        "    # 4. Update learning rate scheduler\n",
        "    # 5. Track metrics (accumulate loss and accuracy)\n",
        "    # 6. Update progress bar with current metrics\n",
        "    #\n",
        "    # You may use tqdm for progress bar and skip None batches\n",
        "    # Return: average_loss, average_accuracy\n",
        "\n",
        "    pass  # TODO: Replace with your implementation\n",
        "\n",
        "\n",
        "def validate(model, dataloader, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    # TODO: Implement validation loop\n",
        "    #\n",
        "    # Similar to training but:\n",
        "    # - Use model.eval() and torch.no_grad()\n",
        "    # - No gradient computation or weight updates\n",
        "    # - Only track loss and accuracy\n",
        "    #\n",
        "    # Return: average_loss, average_accuracy\n",
        "\n",
        "    pass  # TODO: Replace with your implementation\n",
        "\n",
        "print(\"Training functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vfcJuh0w4M1"
      },
      "outputs": [],
      "source": [
        "from hw4_util import get_warmup_cosine_scheduler, save_checkpoint\n",
        "\n",
        "# TODO: Setup training configuration\n",
        "#\n",
        "# Tasks:\n",
        "# 1. Create AdamW optimizer (lr around 2e-4, weight_decay around 0.01)\n",
        "# 2. Calculate warmup steps (e.g., 10% of total)\n",
        "# 3. Create scheduler using get_warmup_cosine_scheduler\n",
        "#\n",
        "# The scheduler warms up learning rate then decays with cosine\n",
        "\n",
        "pass  # TODO: Replace with your implementation\n",
        "\n",
        "# Training info\n",
        "steps_per_epoch = len(train_loader)\n",
        "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "\n",
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# Train for a few epochs (increase for better results)\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = validate(model, val_loader, device)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Get current learning rate\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.2e}\")\n",
        "\n",
        "    # TODO: Save checkpoint when validation improves\n",
        "    #\n",
        "    # If val_loss < best_val_loss:\n",
        "    # - Update best_val_loss\n",
        "    # - Save checkpoint using save_checkpoint function\n",
        "    # - Print confirmation message\n",
        "\n",
        "    pass  # TODO: Replace with checkpoint logic\n",
        "\n",
        "print(\"\\nTraining completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W70V1R6jw4M1"
      },
      "source": [
        "## Part 7: Inference and Evaluation\n",
        "\n",
        "In this section, you will test your trained model by synthesizing speech.\n",
        "\n",
        "### Tasks:\n",
        "1. Load the best checkpoint\n",
        "2. Generate speech from text\n",
        "3. Support voice cloning with prompt audio\n",
        "\n",
        "**TODO:** Load your trained checkpoint.\n",
        "\n",
        "**TODO:** Generate audio samples and evaluate quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-_WZ7aNw4M1"
      },
      "outputs": [],
      "source": [
        "# Inference utilities\n",
        "from hw4_util import load_trained_model, synthesize\n",
        "\n",
        "# Load pretrained components for inference\n",
        "print(\"Loading pretrained components...\")\n",
        "flow_model = load_flow_model(model_dir, device)\n",
        "vocoder = load_vocoder(model_dir, device)\n",
        "\n",
        "# TODO: Load your trained model\n",
        "#\n",
        "# Check if checkpoint exists at Config.RESULTS_DIR/best.pt\n",
        "# If yes: load using load_trained_model in hw4_util.py (needs path, vocab sizes, LM, device)\n",
        "# If no: use current model state\n",
        "\n",
        "trained_model = model  # TODO: Replace with checkpoint loading\n",
        "\n",
        "# Test texts\n",
        "test_texts = [\n",
        "    \"Hello, this is a test of the trained model.\",\n",
        "    \"Text to speech synthesis with language models.\",\n",
        "    \"CosyVoice two is a powerful speech synthesis system.\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting TTS synthesis...\")\n",
        "\n",
        "# TODO: Generate audio for test texts\n",
        "#\n",
        "# For each text:\n",
        "# - Use synthesize function to generate audio\n",
        "# - Save to Config.RESULTS_DIR/synthesized_{i}.wav\n",
        "# - Display audio using IPython.display.Audio\n",
        "#\n",
        "# synthesize returns (audio_tensor, sample_rate)\n",
        "\n",
        "pass  # TODO: Replace with synthesis code\n",
        "\n",
        "print(\"\\nInference completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53VPBFAlw4M1"
      },
      "source": [
        "## Part 8: Voice Cloning and Final Evaluation\n",
        "\n",
        "This is the final evaluation section where you will record your voice directly in the notebook and use it for voice cloning. You will generate 200 utterances with your cloned voice and evaluate the quality using Whisper ASR.\n",
        "\n",
        "**TODO:** Record your voice prompt.\n",
        "\n",
        "**TODO:** Run the voice cloning evaluation.\n",
        "\n",
        "**TODO:** Submit results to Gradescope (included in evaluation above).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNTSZ6E8w4M1"
      },
      "source": [
        "### Step 1: Record Your Voice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJ4CbLa8w4M1"
      },
      "outputs": [],
      "source": [
        "from hw4_util import record_or_load_voice\n",
        "\n",
        "# Record or load existing voice\n",
        "prompt_audio, prompt_text, recorded_file_path = record_or_load_voice(\n",
        "    prompt_text=\"The quick brown fox jumps over the lazy dog.\",\n",
        "    results_dir=Config.RESULTS_DIR\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a68pKbaiw4M1"
      },
      "source": [
        "### Step 2: Test Voice Cloning with One Utterance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJwFhvi3w4M1"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"TESTING VOICE CLONING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test with a single utterance first\n",
        "# You may play around with the test text to see how the model performs\n",
        "test_text = \"This is my cloned voice speaking.\"\n",
        "print(f\"\\n Test text: '{test_text}'\")\n",
        "print(\"Generating with your voice...\")\n",
        "\n",
        "# Generate with voice cloning\n",
        "audio_test, sr = synthesize(\n",
        "    text=test_text,\n",
        "    model=trained_model,\n",
        "    text_tokenizer=text_tokenizer,\n",
        "    speech_tokenizer=speech_tokenizer,\n",
        "    flow_model=flow_model,\n",
        "    vocoder=vocoder,\n",
        "    device=device,\n",
        "    prompt_audio=prompt_audio,\n",
        "    prompt_text=prompt_text,\n",
        "    output_path=f'{Config.RESULTS_DIR}/voice_clone_test.wav'\n",
        ")\n",
        "\n",
        "print(\"\\n Generated audio with your cloned voice:\")\n",
        "display(Audio(audio_test.cpu().numpy(), rate=sr))\n",
        "\n",
        "print(\"\\n Voice cloning test completed!\")\n",
        "print(\"If this sounds like your voice, proceed to the next cell.\")\n",
        "print(\"If not, try recording again with clearer pronunciation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BbqrvJSw4M1"
      },
      "source": [
        "### Step 3: Large-Scale Evaluation with ASR\n",
        "\n",
        "**Automated evaluation system:**\n",
        "- All students evaluate the same 200 texts\n",
        "- Text order shuffled by Student ID (prevents cheating)\n",
        "- Submit to Gradescope for WER scoring\n",
        "\n",
        "**Process:**\n",
        "1. Enter your Student ID → unique shuffle\n",
        "2. Generate 200 utterances (~10-15 min)\n",
        "3. Transcribe with Whisper (~5-10 min)\n",
        "4. Submit `submission_[ID].txt` to Gradescope\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjUjHuFuw4M1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Load pre-generated fixed test set\n",
        "# (All students use the same 200 texts)\n",
        "fixed_test_path = f'{Config.DATA_CACHE_DIR}/fixed_test_set.pt'\n",
        "test_data = torch.load(fixed_test_path)\n",
        "\n",
        "FIXED_TEST_TEXTS = test_data['texts']\n",
        "FIXED_TEST_INDICES = test_data['indices']\n",
        "\n",
        "print(f\"Loaded {len(FIXED_TEST_TEXTS)} test texts from {fixed_test_path}\")\n",
        "print(f\"Seed: {test_data['seed']}\")\n",
        "print(f\"Distribution: {test_data['n_short']} short + {test_data['n_medium']} medium + {test_data['n_long']} long\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD8hExetw4M1"
      },
      "source": [
        "### Step 4: Voice Cloning Evaluation (200 Utterances)\n",
        "\n",
        "**Three-step process:**\n",
        "\n",
        "1. **Load models:** Text tokenizer, speech tokenizer, flow model, vocoder\n",
        "2. **Enter Student ID:** Generates unique text shuffling seed\n",
        "3. **Run evaluation:**\n",
        "   - Generate 200 utterances with your voice (~10-15 min)\n",
        "   - Transcribe with Whisper ASR (~5-10 min)\n",
        "   - Create Gradescope submission file\n",
        "\n",
        "**Note:** WER score visible only on Gradescope after submission.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJiea6HIw4M2"
      },
      "outputs": [],
      "source": [
        "# Voice Cloning Evaluation\n",
        "# Implementation details are in hw4_util.py\n",
        "\n",
        "from hw4_util import load_pretrained_models_for_inference, run_voice_cloning_evaluation\n",
        "\n",
        "# Step 1: Load pretrained models\n",
        "print(\"Step 1: Loading pretrained models...\\n\")\n",
        "models = load_pretrained_models_for_inference(\n",
        "    pretrained_dir=Config.PRETRAINED_DIR,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Extract models\n",
        "text_tokenizer = models['text_tokenizer']\n",
        "speech_tokenizer = models['speech_tokenizer']\n",
        "flow_model = models['flow_model']\n",
        "vocoder = models['vocoder']\n",
        "\n",
        "# Step 2: Get student ID\n",
        "print(\"\\nStep 2: Student identification\\n\")\n",
        "STUDENT_ID = input(\"Enter your Student ID: \").strip()\n",
        "\n",
        "print(f\" Student ID: {STUDENT_ID}\")\n",
        "\n",
        "# Step 3: Run evaluation\n",
        "print(\"\\nStep 3: Running voice cloning evaluation...\\n\")\n",
        "submission_path = run_voice_cloning_evaluation(\n",
        "    student_id=STUDENT_ID,\n",
        "    trained_model=trained_model,\n",
        "    text_tokenizer=text_tokenizer,\n",
        "    speech_tokenizer=speech_tokenizer,\n",
        "    flow_model=flow_model,\n",
        "    vocoder=vocoder,\n",
        "    prompt_audio=prompt_audio,\n",
        "    prompt_text=prompt_text,\n",
        "    fixed_test_set_path=f'{Config.DATA_CACHE_DIR}/fixed_test_set.pt',\n",
        "    results_dir=Config.RESULTS_DIR,\n",
        "    pretrained_dir=Config.PRETRAINED_DIR,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"\\n Done! Upload {submission_path} to Gradescope\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2LHd0VZw4M2"
      },
      "source": [
        "## Part 9: Final Submission Checklist\n",
        "\n",
        "Before submitting to Gradescope, ensure you have completed the following:\n",
        "\n",
        "### Required Files:\n",
        "\n",
        "1. **`submission_[YOUR_ID].txt`** - Auto-generated evaluation file from Part 8\n",
        "   - Generated automatically when you complete Part 8\n",
        "   - Contains your Student ID, seed, and 200 transcriptions\n",
        "   - Do NOT modify this file\n",
        "\n",
        "2. **`hw4-c.pdf`** - PDF export of this notebook\n",
        "\n",
        "### Submission Instructions:\n",
        "1. Complete Part 8 to generate `submission_[YOUR_ID].txt`\n",
        "2. Export this notebook to PDF\n",
        "3. Upload both files to Gradescope\n",
        "\n",
        "### Notes:\n",
        "* Your WER score will be calculated automatically upon submission to GradScope\n",
        "* The points distribution based on WER on the test set:\n",
        "\n",
        "   - < 70% : 15 points\n",
        "\n",
        "   - < 50% : 20 points\n",
        "\n",
        "   - < 45% : 25 points\n",
        "\n",
        "   - < 40% : 30 points\n",
        "\n",
        "   - < 35% : 35 points\n",
        "\n",
        "   - < 30% : 40 points (Full points)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cosyvoice",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}